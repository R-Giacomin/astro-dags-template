from __future__ import annotations
from airflow.decorators import dag, task
from airflow.operators.python import get_current_context
import pendulum
import requests
import pandas as pd
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from datetime import timedelta

# Configurações
GCP_PROJECT = "gen-lang-client-0010767843"
BQ_DATASET = "fda"
BQ_TABLE = "fda_tabaco_reported"
BQ_LOCATION = "US"
GCP_CONN_ID = "google_cloud_default"

API_BASE_URL = "https://api.fda.gov/tobacco/problem.json"
API_LIMIT = 100
MAX_RECORDS_PER_RUN = 1000

DEFAULT_ARGS = {
    "email_on_failure": True,
    "owner": "Generated by Gemini",
    "retries": 3,
    "retry_delay": timedelta(seconds=15),
}

def extract_health_problems(record):
    """
    Extrai os problemas de saúde reportados de um registro da FDA.
    Gera várias linhas caso haja mais de um problema no mesmo report.
    """
    try:
        date_submitted = record.get("date_submitted", "")
        problems = record.get("reported_health_problems", [])
        if not isinstance(problems, list):
            problems = [problems]

        rows = []
        for problem in problems:
            rows.append({
                "date_submitted": date_submitted,
                "reported_health_problems": str(problem).strip(),
                "cases_number": 1
            })
        return rows
    except Exception:
        return []

@task
def fetch_and_load_tobacco_data():
    ctx = get_current_context()
    target_date = ctx["data_interval_start"]

    # Consulta mensal (exemplo: 2025-01-01 até 2025-02-01)
    start_date = target_date.strftime('%Y-%m-01')
    next_month = (target_date.add(months=1)).strftime('%Y-%m-01')

    print(f"🔍 Buscando dados de Tabaco de {start_date} até {next_month}")

    all_results = []
    skip = 0

    while True:
        if skip >= MAX_RECORDS_PER_RUN:
            print(f"⚠️ Limite de {MAX_RECORDS_PER_RUN} registros atingido para {start_date}.")
            break

        params = {
            "search": f"date_submitted:[{start_date}+TO+{next_month}]",
            "limit": API_LIMIT,
            "skip": skip
        }

        try:
            response = requests.get(API_BASE_URL, params=params, timeout=30)
            print(f"📡 Request {skip//API_LIMIT + 1}, status {response.status_code}")

            # 👉 Tratar 400, 404 e 500 como "sem dados"
            if response.status_code in [400, 404, 500]:
                print(f"🔎 Status {response.status_code}: Sem dados para {start_date}.")
                break

            response.raise_for_status()
            data = response.json()
            results = data.get("results", [])

            if not results:
                break

            all_results.extend(results)

            if len(results) < API_LIMIT:
                break

            skip += API_LIMIT

        except Exception as e:
            print(f"❌ Erro na requisição: {e}")
            raise

    print(f"🎯 Total bruto coletado: {len(all_results)} registros")

    if not all_results:
        return "No data"

    # Expandir registros em múltiplas linhas (um problema por linha)
    extracted_data = []
    for record in all_results:
        extracted_data.extend(extract_health_problems(record))

    df = pd.DataFrame(extracted_data)

    if df.empty:
        print("⚠️ Nenhum dado após transformação")
        return "No data after cleaning"

    # Normalizar data
    df['date_submitted'] = pd.to_datetime(df['date_submitted'], errors='coerce').dt.date
    df = df.dropna(subset=['date_submitted'])

    print(f"📊 Linhas finais: {len(df)}")

    # Carregar para BigQuery
    try:
        bq_hook = BigQueryHook(gcp_conn_id=GCP_CONN_ID, location=BQ_LOCATION, use_legacy_sql=False)
        credentials = bq_hook.get_credentials()
        destination_table = f"{BQ_DATASET}.{BQ_TABLE}"

        table_schema = [
            {"name": "date_submitted", "type": "DATE"},
            {"name": "reported_health_problems", "type": "STRING"},
            {"name": "cases_number", "type": "INTEGER"},
        ]

        print(f"🚀 Carregando {len(df)} linhas para BigQuery...")
        df.to_gbq(
            destination_table=destination_table,
            project_id=GCP_PROJECT,
            if_exists="append",
            credentials=credentials,
            table_schema=table_schema,
            location=BQ_LOCATION,
            progress_bar=False,
        )
        print(f"✅ Carga concluída!")
        return f"Successfully loaded {len(df)} records for {start_date}"

    except Exception as e:
        print(f"❌ Erro no BigQuery: {e}")
        raise e

@dag(
    default_args=DEFAULT_ARGS,
    dag_id='fda_tobacco_monthly',
    start_date=pendulum.datetime(2025, 1, 1, tz="UTC"),
    schedule='@monthly',
    catchup=True,
    max_active_runs=1,
    tags=['fda', 'tobacco', 'bigquery', 'monthly'],
)
def fda_tobacco_monthly_dag():
    fetch_and_load_tobacco_data()

dag = fda_tobacco_monthly_dag()
