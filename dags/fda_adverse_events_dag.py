from __future__ import annotations
from airflow.decorators import dag, task
from airflow.operators.python import get_current_context
import pendulum
import requests
import pandas as pd
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

# Configurações
GCP_PROJECT = "gen-lang-client-0010767843"
BQ_DATASET = "fda"
BQ_TABLE = "fda_aspirin_events"
# ✅ CORREÇÃO: Usar a localização correta do BigQuery (seu projeto mostra 'US')
BQ_LOCATION = "US" 
GCP_CONN_ID = "google_cloud_default"

API_BASE_URL = "https://api.fda.gov/drug/event.json"
API_LIMIT = 50  # Limite por requisição
MAX_RECORDS_PER_RUN = 500 # Limite total por dia (ajuste se necessário)

DEFAULT_ARGS = {
    "email_on_failure": True,
    "owner": "Generated by Gemini",
}

def extract_specific_fields(record):
    """
    Extrai campos relevantes de um registro da FDA.
    """
    try:
        # Tenta extrair 'reactionmeddrapt' de forma mais segura
        reaction = record.get("patient", {}).get("reaction", [{}])
        reactionmeddrapt = reaction[0].get("reactionmeddrapt", "") if reaction else ""
        
        return {
            "safetyreportid": str(record.get("safetyreportid", "")),
            "receivedate": record.get("receivedate"),
            # Converte para int de forma segura, tratando None/string vazia como 0
            "serious": int(record.get("serious", 0)) if str(record.get("serious")).isdigit() else 0,
            "patient_patientsex": (
                int(record.get("patient", {}).get("patientsex", 0)) 
                if "patient" in record and str(record["patient"].get("patientsex")).isdigit() 
                else 0
            ),
            "reactionmeddrapt": reactionmeddrapt
        }
    except Exception as e:
        print(f"❌ Erro ao extrair campos: {e}")
        return None

@task
def fetch_and_load_fda_data():
    ctx = get_current_context()
    # Usa data_interval_start para garantir que cada run processe um dia diferente
    target_date = ctx["data_interval_start"] 

    start_date = target_date.strftime('%Y%m%d')
    end_date   = target_date.strftime('%Y%m%d')

    print(f"🔍 Buscando dados de Aspirin para o dia: {start_date}")

    all_results = []
    skip = 0

    while True:
        # Limite de segurança para evitar runs muito longas
        if skip >= MAX_RECORDS_PER_RUN:
            print(f"⚠️ Limite de {MAX_RECORDS_PER_RUN} registros atingido para o dia {start_date}.")
            break

        params = {
            # Filtro de data correto, garantindo busca de um único dia por run
            "search": f'patient.drug.medicinalproduct:"aspirin"+AND+receivedate:[{start_date}+TO+{end_date}]',
            "limit": API_LIMIT,
            "skip": skip
        }

        try:
            response = requests.get(API_BASE_URL, params=params, timeout=30)
            print(f"📡 Request {skip//API_LIMIT + 1}, status {response.status_code}")
            response.raise_for_status() # Lança erro para status codes 4xx/5xx

            data = response.json()
            results = data.get("results", [])

            print(f"🔎 {len(results)} registros nesta página")

            if not results:
                break

            all_results.extend(results)

            if len(results) < API_LIMIT:
                break

            skip += API_LIMIT

        except requests.exceptions.HTTPError as http_err:
            print(f"❌ Erro HTTP na requisição: {http_err}")
            # Se for erro 404/400 (ex: data sem dados), pode continuar
            if response.status_code in [404, 400]: 
                break
            raise # Relança outros erros
        except Exception as e:
            print(f"❌ Erro na requisição: {e}")
            raise # Garante que o Airflow marque a tarefa como falha

    print(f"🎯 Total bruto coletado: {len(all_results)} registros")

    if not all_results:
        print(f"⚠️ Nenhum dado retornado para o dia {start_date}.")
        return "No data"

    extracted_data = [
        extract_specific_fields(record)
        for record in all_results
        if extract_specific_fields(record)
    ]

    df = pd.DataFrame(extracted_data)

    # --- Pré-processamento e Limpeza de Tipos ---
    
    # Função segura para conversão (mantida por segurança)
    def safe_convert_to_int(value):
        try:
            if value is None or pd.isna(value):
                return 0
            # Tenta converter para float (para lidar com strings como '1.0') e depois para int
            return int(float(value))
        except (ValueError, TypeError):
            return 0

    # Aplica conversões e limpeza
    if 'receivedate' in df.columns:
        # ✅ CORREÇÃO: Converte para datetime e normaliza para DATE (sem hora)
        df['receivedate'] = pd.to_datetime(df['receivedate'], format='%Y%m%d', errors='coerce').dt.normalize()
        # Remove linhas onde a data é crucial e está nula (NaT)
        df = df.dropna(subset=['receivedate'])
        
    # Aplica conversões de int
    if 'serious' in df.columns:
        df['serious'] = df['serious'].apply(safe_convert_to_int)
    if 'patient_patientsex' in df.columns:
        df['patient_patientsex'] = df['patient_patientsex'].apply(safe_convert_to_int)
    
    # Aplica conversões de string
    if 'safetyreportid' in df.columns:
        df['safetyreportid'] = df['safetyreportid'].astype(str)
    if 'reactionmeddrapt' in df.columns:
        df['reactionmeddrapt'] = df['reactionmeddrapt'].astype(str).str.strip().fillna("N/A")

    print(f"🔄 Tipos de dados após conversão:")
    print(df.dtypes)
    print(f"📊 Linhas para carregar após limpeza: {len(df)}")

    # Carregar para BigQuery
    if df.empty:
        print("🛑 DataFrame vazio após limpeza. Nada para carregar.")
        return "No data after cleaning"
        
    try:
        bq_hook = BigQueryHook(gcp_conn_id=GCP_CONN_ID, location=BQ_LOCATION, use_legacy_sql=False)
        credentials = bq_hook.get_credentials()
        destination_table = f"{BQ_DATASET}.{BQ_TABLE}"

        # ✅ CORREÇÃO: Schema explícito com 'DATE' para 'receivedate'
        table_schema = [
            {"name": "safetyreportid", "type": "STRING"},
            {"name": "receivedate", "type": "DATE"}, 
            {"name": "serious", "type": "INTEGER"},
            {"name": "patient_patientsex", "type": "INTEGER"},
            {"name": "reactionmeddrapt", "type": "STRING"}
        ]

        print(f"🚀 Carregando {len(df)} linhas para BigQuery em {destination_table}...")
        
        df.to_gbq(
            destination_table=destination_table,
            project_id=GCP_PROJECT,
            if_exists="append", # Usa 'append' para adicionar dados diários
            credentials=credentials,
            table_schema=table_schema,
            location=BQ_LOCATION,
            progress_bar=False,
        )
        print(f"✅ Carga para BigQuery concluída! {len(df)} linhas carregadas.")
        return f"Successfully loaded {len(df)} records for {start_date}"

    except Exception as e:
        # 🛑 REMOÇÃO DO BLOCO 'MINIMAL_TEST':
        # Qualquer falha aqui agora resultará em uma falha da tarefa,
        # expondo o erro real (ex: problema de permissão ou schema)
        print(f"❌ Erro CRÍTICO no BigQuery. A tarefa irá falhar: {e}")
        # Re-lança o erro para que o Airflow marque a tarefa como falha
        raise e 

@dag(
    default_args=DEFAULT_ARGS,
    dag_id='fda_aspirin_daily',
    start_date=pendulum.datetime(2024, 10, 1, tz="UTC"), 
    schedule='@daily',
    catchup=True, # Mantido para buscar dados históricos desde o start_date
    max_active_runs=1,
    tags=['fda', 'aspirin', 'bigquery', 'daily'],
)
def fda_aspirin_daily_dag():
    fetch_and_load_fda_data()

dag = fda_aspirin_daily_dag()
