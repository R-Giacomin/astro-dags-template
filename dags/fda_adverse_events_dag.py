from __future__ import annotations
from airflow.decorators import dag, task
from airflow.operators.python import get_current_context
import pendulum
import requests
import pandas as pd
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

# Configurações
GCP_PROJECT = "gen-lang-client-0010767843" 
BQ_DATASET  = "fda"     
BQ_TABLE    = "fda_aspirin_events"
BQ_LOCATION = "US"      
GCP_CONN_ID = "google_cloud_default"

API_BASE_URL = "https://api.fda.gov/drug/event.json"
API_LIMIT = 100
API_MAX_RECORDS = 10000

DEFAULT_ARGS = {
    "email_on_failure": True,
    "owner": "Generated by Gemini",
}

def extract_specific_fields(record):
    """
    Extrai os campos específicos de cada registro da FDA.
    """
    try:
        # Campos básicos
        safetyreportid = record.get('safetyreportid')
        receivedate = record.get('receivedate')
        serious = record.get('serious')
        
        # Campos aninhados - paciente
        patient_sex = None
        if 'patient' in record and record['patient']:
            patient_sex = record['patient'].get('patientsex')
        
        # Campos aninhados - reações (pegar a primeira reação)
        reactionmeddrapt = None
        if 'patient' in record and record['patient']:
            if 'reaction' in record['patient'] and record['patient']['reaction']:
                # Pegar a primeira reação da lista
                first_reaction = record['patient']['reaction'][0]
                reactionmeddrapt = first_reaction.get('reactionmeddrapt')
        
        return {
            'safetyreportid': safetyreportid,
            'receivedate': receivedate,
            'serious': serious,
            'patient_patientsex': patient_sex,
            'reactionmeddrapt': reactionmeddrapt
        }
    except Exception as e:
        print(f"❌ Erro ao extrair campos do registro: {e}")
        return None

@task
def fetch_and_load_fda_data():
    """
    Busca dados de eventos adversos para Aspirin e extrai colunas específicas.
    """
    ctx = get_current_context()

    # 1. Definir período de busca
    data_interval_start = ctx['data_interval_start']
    data_interval_end = ctx['data_interval_end']
    
    print(f"📅 Data Interval Start: {data_interval_start}")
    print(f"📅 Data Interval End: {data_interval_end}")
    
    # Ajustar para período válido
    if data_interval_start.year > 2024:
        start_date_dt = pendulum.datetime(2023, 1, 1, tz="UTC")
        end_date_dt = pendulum.datetime(2023, 12, 31, tz="UTC")
    else:
        start_date_dt = data_interval_start
        end_date_dt = data_interval_end
    
    # Formato de data: AAAAMMDD
    start_date = start_date_dt.strftime('%Y%m%d')
    end_date = end_date_dt.strftime('%Y%m%d')

    print(f"🔍 Buscando dados do Aspirin no intervalo: {start_date} até {end_date}")

    all_results = []
    skip = 0
    total_records_fetched = 0

    # 2. Loop de Paginação
    while True:
        if skip >= API_MAX_RECORDS:
            print(f"📊 Atingido o limite de {API_MAX_RECORDS} registros.")
            break

        # Query para Aspirin
        search_query = f'patient.drug.medicinalproduct:"aspirin"+AND+receivedate:[{start_date}+TO+{end_date}]'
        
        params = {
            'search': search_query,
            'limit': API_LIMIT,
            'skip': skip,
            'sort': 'receivedate:desc'
        }

        try:
            print(f"📡 Fazendo requisição {skip//API_LIMIT + 1}...")
            
            response = requests.get(API_BASE_URL, params=params, timeout=60)
            print(f"📊 Status Code: {response.status_code}")
            
            if response.status_code != 200:
                print(f"❌ Erro HTTP {response.status_code}")
                response.raise_for_status()
            
            data = response.json()

            # Verificar se há erro
            if 'error' in data:
                error_msg = data['error']
                print(f"⚠️ Erro da API: {error_msg}")
                if error_msg.get('code') == 'NOT_FOUND':
                    print("ℹ️ Nenhum dado encontrado para os critérios.")
                    break
                else:
                    raise Exception(f"API Error: {error_msg}")

            results = data.get('results', [])
            
            if not results:
                print("✅ Nenhum resultado adicional encontrado.")
                break

            all_results.extend(results)
            total_records_fetched += len(results)
            print(f"📥 Página {skip//API_LIMIT + 1}: {len(results)} registros. Total: {total_records_fetched}")

            if len(results) < API_LIMIT:
                print("✅ Última página alcançada.")
                break
            
            skip += API_LIMIT

        except requests.exceptions.HTTPError as e:
            print(f"❌ Erro HTTP: {e}")
            break
        except Exception as e:
            print(f"❌ Erro: {e}")
            break

    print(f"🎯 Busca finalizada. Total de {len(all_results)} registros brutos.")

    if not all_results:
        print("⚠️ Nenhum dado retornado para o Aspirin neste período.")
        return "No aspirin data found"

    # 3. Extrair apenas as colunas específicas
    print("🔄 Extraindo colunas específicas dos registros...")
    
    extracted_data = []
    for i, record in enumerate(all_results):
        extracted_record = extract_specific_fields(record)
        if extracted_record:
            extracted_data.append(extracted_record)
    
    print(f"📊 Extraídos {len(extracted_data)} registros com colunas específicas.")

    # Criar DataFrame apenas com as colunas desejadas
    df = pd.DataFrame(extracted_data)
    
    # Mostrar preview dos dados
    if not df.empty:
        print("👀 Preview do DataFrame:")
        print(df.head())
        print(f"📋 Colunas: {df.columns.tolist()}")
        print(f"📊 Shape: {df.shape}")
        
        # Estatísticas básicas
        print("📈 Estatísticas:")
        for col in df.columns:
            non_null = df[col].notna().sum()
            print(f"  {col}: {non_null} valores não nulos")
    else:
        print("⚠️ DataFrame vazio após extração.")
        return "No data extracted"

    # 4. Processar tipos de dados
    if 'receivedate' in df.columns:
        df['receivedate'] = pd.to_datetime(df['receivedate'], format='%Y%m%d', errors='coerce')
        print("🗓️ Coluna receivedate convertida para datetime.")
    
    if 'serious' in df.columns:
        # Converter para inteiro (1 para serious, 0 para não serious)
        df['serious'] = df['serious'].astype(int)
        print("🔴 Coluna serious convertida para inteiro.")

    # 5. Carregamento para BigQuery
    try:
        bq_hook = BigQueryHook(gcp_conn_id=GCP_CONN_ID, location=BQ_LOCATION, use_legacy_sql=False)
        credentials = bq_hook.get_credentials()
        destination_table = f"{BQ_DATASET}.{BQ_TABLE}"

        # Schema explícito para BigQuery
        table_schema = [
            {"name": "safetyreportid", "type": "STRING"},
            {"name": "receivedate", "type": "TIMESTAMP"},
            {"name": "serious", "type": "INTEGER"},
            {"name": "patient_patientsex", "type": "INTEGER"},
            {"name": "reactionmeddrapt", "type": "STRING"}
        ]

        df.to_gbq(
            destination_table=destination_table,
            project_id=GCP_PROJECT,
            if_exists="append",
            credentials=credentials,
            table_schema=table_schema,
            location=BQ_LOCATION,
            progress_bar=False,
        )
        print(f"✅ Carga para BigQuery concluída! {len(df)} linhas carregadas.")
        return f"Successfully loaded {len(df)} records with specific columns"

    except Exception as e:
        print(f"❌ Erro no BigQuery: {e}")
        raise

@dag(
    default_args=DEFAULT_ARGS,
    dag_id='fda_aspirin_events_weekly',
    start_date=pendulum.datetime(2023, 1, 1, tz="UTC"),
    schedule='@weekly',
    catchup=True,
    max_active_runs=1,
    tags=['fda', 'aspirin', 'bigquery', 'api'],
)
def fda_aspirin_events_dag():
    fetch_and_load_fda_data()

dag = fda_aspirin_events_dag()
